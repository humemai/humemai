{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# with open(\"./wikidata/entityid2label.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "#     entityid2label = json.load(f)  \n",
    "\n",
    "# with open(\"./wikidata/entity_instance_of.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "#     entity_instance_of = json.load(f)\n",
    "\n",
    "with open(\"./wikidata/entity_subclass_of.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    entity_subclass_of = json.load(f)\n",
    "\n",
    "print(f\"{len(entity_subclass_of)} entities in entity_subclass_of\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: JanusGraph Manager - Add and Delete Data with Streaming\n",
    "\n",
    "import nest_asyncio\n",
    "import asyncio\n",
    "import logging\n",
    "import random\n",
    "from itertools import islice\n",
    "from gremlin_python.driver.client import Client\n",
    "from gremlin_python.driver.protocol import GremlinServerError\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Apply nest_asyncio to allow nested event loops (useful in Jupyter notebooks)\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Configure Logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize Gremlin Client\n",
    "GREMLIN_SERVER_URL = \"ws://localhost:8282/gremlin\"  # Update if different\n",
    "client = Client(GREMLIN_SERVER_URL, \"g\")\n",
    "\n",
    "\n",
    "def data_generator(entity_dict, slice_size=10000):\n",
    "    \"\"\"\n",
    "    Generator to yield data slices from the entity_instance_of dictionary.\n",
    "\n",
    "    Args:\n",
    "        entity_dict (dict): Dictionary mapping child labels to parent labels.\n",
    "        slice_size (int): Number of entities to process per slice.\n",
    "\n",
    "    Yields:\n",
    "        tuple: (set of unique labels, list of (child_label, parent_label) tuples)\n",
    "    \"\"\"\n",
    "    it = iter(entity_dict.items())\n",
    "    while True:\n",
    "        slice_data = dict(islice(it, slice_size))\n",
    "        if not slice_data:\n",
    "            break\n",
    "        unique_labels = set()\n",
    "        relationships = []\n",
    "        for child_label, parent_labels in slice_data.items():\n",
    "            unique_labels.add(child_label)\n",
    "            for parent_label in parent_labels:\n",
    "                unique_labels.add(parent_label)\n",
    "                relationships.append((child_label, parent_label))\n",
    "        yield unique_labels, relationships\n",
    "\n",
    "\n",
    "async def submit_query(query, retries=10, backoff_in_seconds=1):\n",
    "    \"\"\"\n",
    "    Submit a Gremlin query with retry logic upon encountering lock contention.\n",
    "\n",
    "    Args:\n",
    "        query (str): The Gremlin query string.\n",
    "        retries (int): Number of retries upon failure.\n",
    "        backoff_in_seconds (int): Initial backoff duration.\n",
    "\n",
    "    Returns:\n",
    "        ResultSet: The result of the query.\n",
    "    \"\"\"\n",
    "    attempt = 0\n",
    "    while attempt <= retries:\n",
    "        try:\n",
    "            future = client.submitAsync(query)\n",
    "            result = await asyncio.wrap_future(future)\n",
    "            logger.debug(f\"Executed query: {query.strip()}\")\n",
    "            return result\n",
    "        except GremlinServerError as e:\n",
    "            error_message = e.message.lower()\n",
    "            if (\n",
    "                \"lock\" in error_message\n",
    "                or \"contention\" in error_message\n",
    "                or \"expected value mismatch\" in error_message\n",
    "            ):\n",
    "                if attempt < retries:\n",
    "                    sleep_time = backoff_in_seconds * (2**attempt) + random.uniform(\n",
    "                        0, 1\n",
    "                    )\n",
    "                    logger.warning(\n",
    "                        f\"Lock contention detected. Retrying in {sleep_time:.2f} seconds... \"\n",
    "                        f\"(Attempt {attempt + 1}/{retries})\"\n",
    "                    )\n",
    "                    await asyncio.sleep(sleep_time)\n",
    "                    attempt += 1\n",
    "                else:\n",
    "                    logger.error(f\"Exceeded maximum retries for query: {query.strip()}\")\n",
    "                    raise\n",
    "            else:\n",
    "                logger.error(f\"Gremlin Server Error: {e.message}\")\n",
    "                raise\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Unexpected error: {e}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "async def upsert_vertex(label):\n",
    "    \"\"\"\n",
    "    Insert a vertex with the given label if it doesn't already exist.\n",
    "\n",
    "    Args:\n",
    "        label (str): The unique label of the entity.\n",
    "    \"\"\"\n",
    "    query = f\"\"\"\n",
    "        g.V().hasLabel('{label}').fold().\n",
    "        coalesce(\n",
    "            unfold(),\n",
    "            addV('{label}')\n",
    "        )\n",
    "    \"\"\"\n",
    "    await submit_query(query)\n",
    "\n",
    "\n",
    "async def upsert_edge(child_label, parent_label):\n",
    "    \"\"\"\n",
    "    Insert an edge 'subclass_of' from child to parent if it doesn't already exist.\n",
    "\n",
    "    Args:\n",
    "        child_label (str): Label of the child entity.\n",
    "        parent_label (str): Label of the parent entity.\n",
    "    \"\"\"\n",
    "    query = f\"\"\"\n",
    "        g.V().hasLabel('{child_label}').as('child').\n",
    "        V().hasLabel('{parent_label}').as('parent').\n",
    "        coalesce(\n",
    "            __.select('child').outE('subclass_of').where(__.inV().as('parent')),\n",
    "            __.select('child').addE('subclass_of').to('parent')\n",
    "        )\n",
    "    \"\"\"\n",
    "    await submit_query(query)\n",
    "\n",
    "\n",
    "async def process_vertices(unique_labels, max_workers=3):\n",
    "    \"\"\"\n",
    "    Process and insert all unique vertices with controlled concurrency.\n",
    "\n",
    "    Args:\n",
    "        unique_labels (set): Set of unique entity labels.\n",
    "        max_workers (int): Number of concurrent tasks for processing.\n",
    "    \"\"\"\n",
    "    semaphore = asyncio.Semaphore(max_workers)\n",
    "\n",
    "    async def sem_upsert_vertex(label):\n",
    "        async with semaphore:\n",
    "            await upsert_vertex(label)\n",
    "\n",
    "    tasks = [asyncio.create_task(sem_upsert_vertex(label)) for label in unique_labels]\n",
    "\n",
    "    # Await all tasks and handle exceptions\n",
    "    for f in tqdm(\n",
    "        asyncio.as_completed(tasks), total=len(tasks), desc=\"Upserting Vertices\"\n",
    "    ):\n",
    "        try:\n",
    "            await f\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during vertex upsert: {e}\")\n",
    "\n",
    "\n",
    "async def process_edges(relationships, batch_size=5, max_workers=3):\n",
    "    \"\"\"\n",
    "    Process and insert all edges in batches with controlled concurrency.\n",
    "\n",
    "    Args:\n",
    "        relationships (list): List of (child, parent) tuples.\n",
    "        batch_size (int): Number of relationships to process per batch.\n",
    "        max_workers (int): Number of concurrent tasks for processing.\n",
    "    \"\"\"\n",
    "    total = len(relationships)\n",
    "    logger.info(f\"Total relationships to process: {total}\")\n",
    "\n",
    "    # Split into batches\n",
    "    for i in range(0, total, batch_size):\n",
    "        batch = relationships[i : i + batch_size]\n",
    "        batch_number = i // batch_size + 1\n",
    "        logger.info(\n",
    "            f\"Processing edge batch {batch_number} with {len(batch)} relationships.\"\n",
    "        )\n",
    "        await _process_edge_batch(batch, max_workers)\n",
    "        logger.info(f\"Edge batch {batch_number} processed.\")\n",
    "\n",
    "\n",
    "async def _process_edge_batch(batch, max_workers):\n",
    "    \"\"\"\n",
    "    Process a single batch of edges using limited concurrency.\n",
    "\n",
    "    Args:\n",
    "        batch (list): List of (child, parent) tuples.\n",
    "        max_workers (int): Number of concurrent tasks for processing.\n",
    "    \"\"\"\n",
    "    semaphore = asyncio.Semaphore(max_workers)\n",
    "\n",
    "    async def sem_upsert_edge(child, parent):\n",
    "        async with semaphore:\n",
    "            await upsert_edge(child, parent)\n",
    "\n",
    "    tasks = [\n",
    "        asyncio.create_task(sem_upsert_edge(child, parent)) for child, parent in batch\n",
    "    ]\n",
    "\n",
    "    # Await all tasks and handle exceptions\n",
    "    for f in tqdm(\n",
    "        asyncio.as_completed(tasks), total=len(tasks), desc=\"Upserting Edges\"\n",
    "    ):\n",
    "        try:\n",
    "            await f\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during edge upsert: {e}\")\n",
    "\n",
    "\n",
    "def get_data_stream(entity_dict, slice_size=10000):\n",
    "    \"\"\"\n",
    "    Create a generator to stream data slices from the entity_instance_of dictionary.\n",
    "\n",
    "    Args:\n",
    "        entity_dict (dict): Dictionary mapping child labels to parent labels.\n",
    "        slice_size (int): Number of entities to process per slice.\n",
    "\n",
    "    Yields:\n",
    "        tuple: (set of unique labels, list of (child_label, parent_label) tuples)\n",
    "    \"\"\"\n",
    "    return data_generator(entity_dict, slice_size)\n",
    "\n",
    "\n",
    "async def delete_all_vertices():\n",
    "    \"\"\"\n",
    "    Delete all vertices (and consequently all edges) from the JanusGraph database.\n",
    "    \"\"\"\n",
    "    delete_query = \"g.V().drop().iterate()\"\n",
    "    try:\n",
    "        await submit_query(delete_query)\n",
    "        logger.info(\n",
    "            \"All vertices and their associated edges have been successfully deleted.\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to delete all data: {e}\")\n",
    "\n",
    "\n",
    "async def delete_vertices_in_batches(batch_size=100):\n",
    "    \"\"\"\n",
    "    Delete all vertices in smaller batches to minimize load and lock contention.\n",
    "\n",
    "    Args:\n",
    "        batch_size (int): Number of vertices to delete per batch.\n",
    "    \"\"\"\n",
    "    logger.info(\"Starting deletion of vertices in batches...\")\n",
    "    while True:\n",
    "        delete_batch_query = f\"\"\"\n",
    "            g.V().limit({batch_size}).drop().iterate()\n",
    "        \"\"\"\n",
    "        try:\n",
    "            await submit_query(delete_batch_query)\n",
    "            # Check if any vertices remain\n",
    "            count_query = \"g.V().count()\"\n",
    "            result = await submit_query(count_query)\n",
    "            remaining = result.one().value\n",
    "            logger.info(\n",
    "                f\"Deleted a batch of {batch_size} vertices. Remaining vertices: {remaining}\"\n",
    "            )\n",
    "            if remaining == 0:\n",
    "                logger.info(\"All vertices have been successfully deleted.\")\n",
    "                break\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to delete a batch of vertices: {e}\")\n",
    "            break\n",
    "\n",
    "\n",
    "async def add_data(\n",
    "    slice_size=10000, max_workers_vertices=3, max_workers_edges=10, batch_size=100\n",
    "):\n",
    "    \"\"\"\n",
    "    Add data to the JanusGraph database by inserting vertices and edges.\n",
    "\n",
    "    Args:\n",
    "        slice_size (int): Number of entities to process per data slice.\n",
    "        max_workers_vertices (int): Number of concurrent tasks for vertex upsertion.\n",
    "        max_workers_edges (int): Number of concurrent tasks for edge upsertion.\n",
    "        batch_size (int): Number of relationships to process per edge batch.\n",
    "    \"\"\"\n",
    "    data_stream = get_data_stream(entity_subclass_of, slice_size)\n",
    "    slice_number = 0\n",
    "    for unique_labels, relationships in data_stream:\n",
    "        slice_number += 1\n",
    "        logger.info(\n",
    "            f\"Processing slice {slice_number} with {len(unique_labels)} unique labels and {len(relationships)} relationships.\"\n",
    "        )\n",
    "\n",
    "        # Process all vertices in the current slice\n",
    "        await process_vertices(unique_labels, max_workers=max_workers_vertices)\n",
    "\n",
    "        # Process all edges in the current slice\n",
    "        await process_edges(\n",
    "            relationships, batch_size=batch_size, max_workers=max_workers_edges\n",
    "        )\n",
    "\n",
    "\n",
    "async def delete_data(batch_size=100, method=\"batch\"):\n",
    "    \"\"\"\n",
    "    Delete all data from the JanusGraph database.\n",
    "\n",
    "    Args:\n",
    "        batch_size (int): Number of vertices to delete per batch (used in batch deletion).\n",
    "        method (str): Deletion method - 'all' or 'batch'.\n",
    "                      'all' deletes all vertices in one operation.\n",
    "                      'batch' deletes vertices in smaller batches.\n",
    "    \"\"\"\n",
    "    if method == \"all\":\n",
    "        # Option 1: Delete all at once\n",
    "        await delete_all_vertices()\n",
    "    elif method == \"batch\":\n",
    "        # Option 2: Delete in batches to minimize lock contention\n",
    "        await delete_vertices_in_batches(batch_size=batch_size)\n",
    "    else:\n",
    "        logger.error(\"Invalid deletion method specified. Choose 'all' or 'batch'.\")\n",
    "\n",
    "\n",
    "async def close_client_async():\n",
    "    \"\"\"\n",
    "    Close the Gremlin client synchronously.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client.close()\n",
    "        logger.info(\"Disconnected from the Gremlin server.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error while closing client: {e}\")\n",
    "\n",
    "\n",
    "async def main_async(\n",
    "    operation=\"add\",\n",
    "    slice_size=10000,\n",
    "    max_workers_vertices=3,\n",
    "    max_workers_edges=10,\n",
    "    batch_size=100,\n",
    "    delete_method=\"batch\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Main asynchronous function to perform add or delete operations based on arguments.\n",
    "\n",
    "    Args:\n",
    "        operation (str): 'add' to add data or 'delete' to delete all data.\n",
    "        slice_size (int): Number of entities to process when adding data.\n",
    "        max_workers_vertices (int): Number of concurrent tasks for vertex upsertion.\n",
    "        max_workers_edges (int): Number of concurrent tasks for edge upsertion.\n",
    "        batch_size (int): Number of relationships per edge batch or number of vertices per deletion batch.\n",
    "        delete_method (str): Method of deletion - 'all' or 'batch'.\n",
    "    \"\"\"\n",
    "    if operation == \"add\":\n",
    "        logger.info(\"Starting data addition process...\")\n",
    "        await add_data(\n",
    "            slice_size=slice_size,\n",
    "            max_workers_vertices=max_workers_vertices,\n",
    "            max_workers_edges=max_workers_edges,\n",
    "            batch_size=batch_size,\n",
    "        )\n",
    "    elif operation == \"delete\":\n",
    "        logger.info(\"Starting data deletion process...\")\n",
    "        await delete_data(batch_size=batch_size, method=delete_method)\n",
    "    else:\n",
    "        logger.error(\"Invalid operation specified. Choose 'add' or 'delete'.\")\n",
    "\n",
    "    # Close the Gremlin connection\n",
    "    await close_client_async()\n",
    "\n",
    "\n",
    "def run_operation(\n",
    "    operation=\"add\",\n",
    "    slice_size=10000,\n",
    "    max_workers_vertices=3,\n",
    "    max_workers_edges=10,\n",
    "    batch_size=100,\n",
    "    delete_method=\"batch\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Run the specified operation asynchronously within the Jupyter notebook.\n",
    "\n",
    "    Args:\n",
    "        operation (str): 'add' to add data or 'delete' to delete all data.\n",
    "        slice_size (int): Number of entities to process when adding data.\n",
    "        max_workers_vertices (int): Number of concurrent tasks for vertex upsertion.\n",
    "        max_workers_edges (int): Number of concurrent tasks for edge upsertion.\n",
    "        batch_size (int): Number of relationships per edge batch or number of vertices per deletion batch.\n",
    "        delete_method (str): Method of deletion - 'all' or 'batch'.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        asyncio.run(\n",
    "            main_async(\n",
    "                operation,\n",
    "                slice_size,\n",
    "                max_workers_vertices,\n",
    "                max_workers_edges,\n",
    "                batch_size,\n",
    "                delete_method,\n",
    "            )\n",
    "        )\n",
    "    except RuntimeError as e:\n",
    "        logger.error(f\"Runtime error: {e}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "# Example Usage:\n",
    "\n",
    "# To add data to the JanusGraph database\n",
    "# Adjust the parameters as needed and uncomment the line below to execute\n",
    "\n",
    "run_operation(\n",
    "    operation=\"add\",\n",
    "    slice_size=10000,  # Number of entities per slice\n",
    "    max_workers_vertices=10,  # Number of concurrent tasks for vertex upsertion\n",
    "    max_workers_edges=10,  # Number of concurrent tasks for edge upsertion\n",
    "    batch_size=1000,  # Number of relationships per edge batch\n",
    ")\n",
    "\n",
    "# To delete all data from the JanusGraph database using batch deletion\n",
    "# Adjust the batch_size and method as needed and uncomment the line below to execute\n",
    "\n",
    "# run_operation(\n",
    "#     operation='delete',\n",
    "#     batch_size=100,                # Number of vertices to delete per batch\n",
    "#     delete_method='batch'          # Deletion method: 'all' or 'batch'\n",
    "# )\n",
    "\n",
    "# To delete all data from the JanusGraph database in one go\n",
    "# Uncomment the lines below to execute\n",
    "\n",
    "# run_operation(\n",
    "#     operation='delete',\n",
    "#     batch_size=0,                   # Not used in 'all' method\n",
    "#     delete_method='all'             # Specify deletion method as 'all'\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "humemai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
