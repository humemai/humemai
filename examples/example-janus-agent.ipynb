{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from humemai.janusgraph import Humemai\n",
    "from humemai.utils import disable_logger\n",
    "\n",
    "disable_logger()\n",
    "\n",
    "humemai = Humemai()\n",
    "humemai.start_containers(warmup_seconds=10)\n",
    "humemai.connect()\n",
    "humemai.remove_all_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "# Open the file in read mode\n",
    "with open('example.txt', 'r') as file:\n",
    "    # Read the contents of the file\n",
    "    content = file.read()\n",
    "\n",
    "# Split the text into chunks based on speaker\n",
    "# Match lines that start with a speaker's name followed by a colon\n",
    "chunks = re.findall(r'^.*?:.*$', content, re.MULTILINE)\n",
    "\n",
    "# Clean up the chunks by stripping unnecessary spaces\n",
    "chunks = [chunk.strip() for chunk in chunks]\n",
    "\n",
    "# Print the chunks\n",
    "for chunk in chunks:\n",
    "    print(chunk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "\n",
    "\n",
    "def get_pipeline(\n",
    "    model: str = \"meta-llama/Llama-3.2-1B-Instruct\",\n",
    "    device: str = \"cpu\",\n",
    "    quantization: str = \"16bit\",\n",
    ") -> transformers.Pipeline:\n",
    "    \"\"\"Get a text generation pipeline with the specified device and quantization.\n",
    "\n",
    "    Args:\n",
    "        model (str): The model to use for text generation. \n",
    "            Defaults to \"meta-llama/Llama-3.2-1B-Instruct\".\n",
    "            meta-llama/Llama-3.2-3B-Instruct, \n",
    "            meta-llama/Llama-3.1-8B-Instruct\n",
    "            ...\n",
    "\n",
    "            are also available.\n",
    "        device (str): The device to run the pipeline on. Defaults to \"cpu\".\n",
    "        quantization (str): The quantization to apply to the model. Defaults to \"16bit\".\n",
    "\n",
    "    Returns:\n",
    "        transformers.Pipeline: The text generation pipeline.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if quantization == \"16bit\":\n",
    "        quantization_config = None\n",
    "    elif quantization == \"8bit\":\n",
    "        quantization_config = {\"load_in_8bit\": True}\n",
    "    elif quantization == \"4bit\":\n",
    "        quantization_config = {\"load_in_4bit\": True}\n",
    "\n",
    "    return transformers.pipeline(\n",
    "        \"text-generation\",\n",
    "        model=\"meta-llama/Llama-3.2-1B-Instruct\",\n",
    "        model_kwargs={\n",
    "            \"torch_dtype\": torch.bfloat16,\n",
    "            \"quantization_config\": quantization_config,\n",
    "        },\n",
    "        device_map=device,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(history: str, next_text: str) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Generate the prompt for the AI assistant to convert text to a knowledge graph.\n",
    "\n",
    "    Args:\n",
    "        history (str): The history of the knowledge graph extracted so far.\n",
    "        next_text (str): The new text to convert into a knowledge graph.\n",
    "\n",
    "    Returns:\n",
    "        list[dict]: A structured prompt for the AI assistant to build a knowledge graph.\n",
    "    \"\"\"\n",
    "    prompt = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"\"\"\n",
    "You are an AI assistant that builds knowledge graphs from text. \n",
    "For each input, you extract entities and relationships from the provided text \n",
    "and convert them into a structured JSON-based knowledge graph.\n",
    "\n",
    "**Important:** You should extract entities and relations from the new text provided.\n",
    "If the new text provides updated information about existing entities or relations \n",
    "(e.g., age change, new attributes), you should output these entities and relations \n",
    "again with the updated information. Do not include entities or relations from the \n",
    "previous history that have not changed.\n",
    "\n",
    "You may use the history to understand context and disambiguate entities.\n",
    "\n",
    "Your output must follow this JSON format:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"entities\": [\n",
    "    {\"label\": \"Entity1\", \"type\": \"Type1\"},\n",
    "    {\"label\": \"Entity2\", \"type\": \"Type2\"}\n",
    "  ],\n",
    "  \"relations\": [\n",
    "    {\"source\": \"Entity1\", \"target\": \"Entity2\", \"relation\": \"RelationName\"}\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "Each entity must have a unique label and type (e.g., \"Person\", \"Company\", \"Object\",\n",
    "\"Event\"). Relations must specify:\n",
    "\n",
    "- `source`: the label of the originating entity,\n",
    "- `target`: the label of the connected entity,\n",
    "- `relation`: the relationship type between the source and target.\n",
    "\n",
    "## Example:\n",
    "\n",
    "### Previous Knowledge Graph History:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"entities\": [\n",
    "    {\"label\": \"Sarah\", \"properties\": {\"type\": \"Person\"}},\n",
    "    {\"label\": \"InnovateAI\", \"properties\": {\"type\": \"Company\"}}\n",
    "  ],\n",
    "  \"relations\": [\n",
    "    {\"source\": \"Sarah\", \"target\": \"InnovateAI\", \"relation\": \"works_at\"}\n",
    "  ]\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "### New Text to Process:\n",
    "\n",
    "\"Sarah, now 30 years old, was promoted to Senior Data Scientist at InnovateAI.\"\n",
    "\n",
    "### Output Knowledge Graph:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"entities\": [\n",
    "    {\"label\": \"Sarah\", \"properties\": {\"type\": \"Person\", \"age\": 30}},\n",
    "    {\"label\": \"InnovateAI\", \"properties\": {\"type\": \"Company\"}},\n",
    "    {\"label\": \"Senior Data Scientist\", \"properties\": {\"type\": \"Position\"}}\n",
    "  ],\n",
    "  \"relations\": [\n",
    "    {\"source\": \"Sarah\", \"target\": \"InnovateAI\", \"relation\": \"works_at\"},\n",
    "    {\"source\": \"Sarah\", \"target\": \"Senior Data Scientist\", \"relation\": \"holds_position\"}\n",
    "  ]\n",
    "}\n",
    "````\n",
    "Note that even though \"Sarah\" and \"InnovateAI\" were already in the history, we included\n",
    "\"Sarah\" again with the updated age and added new relations based on the new information.\n",
    "\n",
    "\n",
    "## Detailed Instructions:\n",
    "\n",
    "- Extract entities and relations from the new text provided.\n",
    "- If the new text provides updated information about existing entities or relations, include these in your output.\n",
    "- Do not include entities or relations from the history that have not changed.\n",
    "- Use the history for context and to disambiguate entities.\n",
    "- Ensure the output adheres strictly to the JSON format specified. \"\"\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Here is the knowledge graph extracted so far: {history}\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Here is the new text to process and incorporate: {next_text}\",\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = get_pipeline(\"meta-llama/Llama-3.2-1B-Instruct\", \"cuda\", \"16bit\")\n",
    "\n",
    "history = {\"entities\": [], \"relations\": []}\n",
    "\n",
    "for chunk in tqdm(chunks):\n",
    "\n",
    "    outputs = pipeline(\n",
    "        generate_prompt(history, chunk),\n",
    "        max_new_tokens=1024,\n",
    "    )\n",
    "    text_content = outputs[0][\"generated_text\"][-1][\"content\"]\n",
    "\n",
    "    json_match = re.search(r\"```json\\n(.*?)\\n```\", text_content, re.DOTALL)\n",
    "\n",
    "    try:\n",
    "        json_text = json_match.group(1)  # Extract JSON content\n",
    "        dict_graph = json.loads(json_text)\n",
    "\n",
    "        # Write short term vertices\n",
    "        for entity in dict_graph[\"entities\"]:\n",
    "            vertex = humemai.write_short_term_vertex(\n",
    "                label=entity[\"label\"], properties=entity[\"properties\"]\n",
    "            )\n",
    "\n",
    "        # Write short term edges\n",
    "        for relation in dict_graph[\"relations\"]:\n",
    "            head_label = relation[\"source\"]\n",
    "            head_vertex = humemai.find_vertex_by_label(head_label)[0]\n",
    "            edge_label = relation[\"relation\"]\n",
    "            tail_label = relation[\"target\"]\n",
    "            tail_vertex = humemai.find_vertex_by_label(tail_label)[0]\n",
    "\n",
    "            edge = humemai.write_short_term_edge(\n",
    "                head_vertex=head_vertex,\n",
    "                edge_label=edge_label,\n",
    "                tail_vertex=tail_vertex,\n",
    "            )\n",
    "\n",
    "        short_term_vertices, long_term_vertices, short_term_edges, long_term_edges = (\n",
    "            humemai.get_working_vertices_and_edges(\n",
    "                short_term_vertices=humemai.get_all_short_term_vertices(),\n",
    "                short_term_edges=humemai.get_all_short_term_edges(),\n",
    "                include_all_long_term=False,\n",
    "                hops=2,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # move to the long-term memory\n",
    "        for vertex in short_term_vertices:\n",
    "            humemai.move_short_term_vertex(vertex, \"episodic\")\n",
    "\n",
    "        for edge in short_term_edges:\n",
    "            humemai.move_short_term_edge(edge, \"episodic\")\n",
    "\n",
    "        # remove all short term vertices and edges\n",
    "        humemai.remove_all_short_term()\n",
    "\n",
    "        entities = []\n",
    "        for vertex in long_term_vertices:\n",
    "            entities.append(\n",
    "                {\n",
    "                    \"label\": vertex.label,\n",
    "                    \"properties\": {\n",
    "                        key: val\n",
    "                        for key, val in humemai.get_vertex_properties(vertex).items()\n",
    "                        if key not in [\"num_recalled\", \"event_time\", \"known_since\"]\n",
    "                    },\n",
    "                }\n",
    "            )\n",
    "\n",
    "        relations = []\n",
    "        for edge in long_term_edges:\n",
    "            relations.append(\n",
    "                {\n",
    "                    \"source\": edge.outV.label,\n",
    "                    \"relation\": edge.label,\n",
    "                    \"target\": edge.inV.label,\n",
    "                }\n",
    "            )\n",
    "        history = {\"entities\": entities, \"relations\": relations}\n",
    "        print(\"history: \", history)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HumemAI --averted--> Crisis | Properties: {'event_time': ['2024-11-20T14:46:35'], 'num_recalled': 0}\n",
      "HumemAI --retracing--> steps | Properties: {'event_time': ['2024-11-20T14:46:35'], 'num_recalled': 0}\n",
      "HumemAI --resolution--> Resolution | Properties: {'event_time': ['2024-11-20T14:46:35'], 'num_recalled': 0}\n",
      "HumemAI --focuses_on--> Keys | Properties: {'num_recalled': 4, 'event_time': ['2024-11-20T14:46:35', '2024-11-20T14:46:35']}\n",
      "Grace --mentions--> keys | Properties: {'event_time': ['2024-11-20T14:46:35'], 'num_recalled': 5}\n",
      "Grace --mentions--> Keys | Properties: {'event_time': ['2024-11-20T14:46:35'], 'num_recalled': 3}\n",
      "Grace --sees--> keys | Properties: {'num_recalled': 5, 'event_time': ['2024-11-20T14:46:35']}\n",
      "Grace --sees--> Keys | Properties: {'event_time': ['2024-11-20T14:46:35'], 'num_recalled': 3}\n",
      "Bob --took--> Keys | Properties: {'num_recalled': 5, 'event_time': ['2024-11-20T14:46:35', '2024-11-20T14:46:35', '2024-11-20T14:46:35']}\n",
      "Bob --took--> Desk | Properties: {'event_time': ['2024-11-20T14:46:35', '2024-11-20T14:46:35', '2024-11-20T14:46:35'], 'num_recalled': 5}\n",
      "Bob --left--> Desk | Properties: {'event_time': ['2024-11-20T14:46:35', '2024-11-20T14:46:35'], 'num_recalled': 5}\n",
      "Bob --tells--> See | Properties: {'num_recalled': 1, 'event_time': ['2024-11-20T14:46:35']}\n",
      "Bob --sees--> See | Properties: {'event_time': ['2024-11-20T14:46:35'], 'num_recalled': 1}\n",
      "Why --asks--> Alice | Properties: {'num_recalled': 4, 'event_time': ['2024-11-20T14:46:35']}\n",
      "Why --asks--> Bob | Properties: {'num_recalled': 1, 'event_time': ['2024-11-20T14:46:35']}\n",
      "Why --speaks--> Alice | Properties: {'num_recalled': 2, 'event_time': ['2024-11-20T14:46:35']}\n",
      "Why --about--> Alice | Properties: {'num_recalled': 4, 'event_time': ['2024-11-20T14:46:35']}\n",
      "Why --tells--> Alice | Properties: {'event_time': ['2024-11-20T14:46:35'], 'num_recalled': 2}\n",
      "Why --tells--> Bob | Properties: {'event_time': ['2024-11-20T14:46:35'], 'num_recalled': 1}\n",
      "Why --tells--> I | Properties: {'num_recalled': 0, 'event_time': ['2024-11-20T14:46:35']}\n",
      "Alice --leaves--> Why | Properties: {'num_recalled': 4, 'event_time': ['2024-11-20T14:46:35']}\n",
      "Alice --retraces--> Charlie | Properties: {'event_time': ['2024-11-20T14:46:35'], 'num_recalled': 4}\n",
      "Alice --said--> Okay | Properties: {'event_time': ['2024-11-20T14:46:35'], 'num_recalled': 3}\n",
      "Alice --came--> in | Properties: {'event_time': ['2024-11-20T14:46:35'], 'num_recalled': 3}\n",
      "Alice --finds--> keys | Properties: {'num_recalled': 2, 'event_time': ['2024-11-20T14:46:35']}\n",
      "Alice --took--> Bob | Properties: {'event_time': ['2024-11-20T14:46:35', '2024-11-20T14:46:35', '2024-11-20T14:46:35', '2024-11-20T14:46:35'], 'num_recalled': 9}\n",
      "Alice --speaks--> Why | Properties: {'event_time': ['2024-11-20T14:46:35', '2024-11-20T14:46:35'], 'num_recalled': 3}\n",
      "Alice --left--> Desk | Properties: {'num_recalled': 9, 'event_time': ['2024-11-20T14:46:35', '2024-11-20T14:46:35', '2024-11-20T14:46:35']}\n",
      "Alice --tells--> Why | Properties: {'event_time': ['2024-11-20T14:46:35', '2024-11-20T14:46:35'], 'num_recalled': 3}\n",
      "Alice --tells--> Charlie | Properties: {'num_recalled': 2, 'event_time': ['2024-11-20T14:46:35']}\n",
      "Alice --has_key--> Keys | Properties: {'num_recalled': 5, 'event_time': ['2024-11-20T14:46:35', '2024-11-20T14:46:35', '2024-11-20T14:46:35']}\n",
      "Charlie --asks--> What's | Properties: {'num_recalled': 5, 'event_time': ['2024-11-20T14:46:35', '2024-11-20T14:46:35']}\n",
      "Charlie --speaks--> What's | Properties: {'num_recalled': 5, 'event_time': ['2024-11-20T14:46:35', '2024-11-20T14:46:35']}\n",
      "Charlie --tells--> What's | Properties: {'event_time': ['2024-11-20T14:46:35'], 'num_recalled': 6}\n",
      "Charlie --mentions--> break room | Properties: {'event_time': ['2024-11-20T14:46:35', '2024-11-20T14:46:35'], 'num_recalled': 3}\n",
      "Desk --owns--> Alice | Properties: {'event_time': ['2024-11-20T14:46:35', '2024-11-20T14:46:35', '2024-11-20T14:46:35', '2024-11-20T14:46:35'], 'num_recalled': 6}\n",
      "What's --asks--> Alice | Properties: {'num_recalled': 4, 'event_time': ['2024-11-20T14:46:35']}\n",
      "What's --about--> Charlie | Properties: {'num_recalled': 5, 'event_time': ['2024-11-20T14:46:35', '2024-11-20T14:46:35']}\n",
      "What's --tells--> What's | Properties: {'event_time': ['2024-11-20T14:46:35'], 'num_recalled': 3}\n"
     ]
    }
   ],
   "source": [
    "for edge in humemai.get_all_long_term_edges():\n",
    "    print(\n",
    "        f\"{edge.outV.label} --{edge.label}--> {edge.inV.label} | Properties: {humemai.get_edge_properties(edge)}\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "humemai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
